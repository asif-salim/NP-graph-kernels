{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import networkx as nx\n",
    "import cupy as cp\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "from collections import OrderedDict\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs to be given\n",
    "dataset = 'BZR' # Choose the datasets among: 'PROTEINS', 'ENZYMES', 'BZR', 'COX2', 'DHFR', 'SYNTHETICnew'\n",
    "iter_num = 3 # No: of WL iteration\n",
    "ker = 1  # choose 0 for Gaussian kernel as base kernel and 1 for linear kernel\n",
    "path_to_data_folder = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'PROTEINS':\n",
    "    A = np.loadtxt(path_to_data_folder + \"PROTEINS/PROTEINS_A.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    indicator = np.loadtxt(path_to_data_folder + \"PROTEINS/PROTEINS_graph_indicator.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    NL = np.loadtxt(path_to_data_folder + \"PROTEINS/PROTEINS_node_labels.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    NA = np.loadtxt(path_to_data_folder + \"PROTEINS/PROTEINS_node_attributes.txt\", dtype = float, delimiter=\",\", unpack=False)\n",
    "    EL = None\n",
    "    EA = None\n",
    "    \n",
    "if dataset == 'ENZYMES':\n",
    "    A = np.loadtxt(path_to_data_folder + \"ENZYMES/ENZYMES_A.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    indicator = np.loadtxt(path_to_data_folder + \"ENZYMES/ENZYMES_graph_indicator.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    NL = np.loadtxt(path_to_data_folder + \"ENZYMES/ENZYMES_node_labels.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    NA = np.loadtxt(path_to_data_folder + \"ENZYMES/ENZYMES_node_attributes.txt\", dtype = float, delimiter=\",\", unpack=False)\n",
    "    EL = None\n",
    "    EA = None\n",
    "    \n",
    "if dataset == 'BZR':\n",
    "    A = np.loadtxt(path_to_data_folder + \"BZR/BZR_A.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    indicator = np.loadtxt(path_to_data_folder + \"BZR/BZR_graph_indicator.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    NL = np.loadtxt(path_to_data_folder + \"BZR/BZR_node_labels.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    NA = np.loadtxt(path_to_data_folder + \"BZR/BZR_node_attributes.txt\", dtype = float, delimiter=\",\", unpack=False)\n",
    "    EL = None\n",
    "    EA = None\n",
    "    \n",
    "if dataset == 'COX2':\n",
    "    A = np.loadtxt(path_to_data_folder + \"COX2/COX2_A.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    indicator = np.loadtxt(path_to_data_folder + \"COX2/COX2_graph_indicator.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    NL = np.loadtxt(path_to_data_folder + \"COX2/COX2_node_labels.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    NA = np.loadtxt(path_to_data_folder + \"COX2/COX2_node_attributes.txt\", dtype = float, delimiter=\",\", unpack=False)\n",
    "    EL = None\n",
    "    EA = None\n",
    "    \n",
    "if dataset == 'DHFR':\n",
    "    A = np.loadtxt(path_to_data_folder + \"DHFR/DHFR_A.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    indicator = np.loadtxt(path_to_data_folder + \"DHFR/DHFR_graph_indicator.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    NL = np.loadtxt(path_to_data_folder + \"DHFR/DHFR_node_labels.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    NA = np.loadtxt(path_to_data_folder + \"DHFR/DHFR_node_attributes.txt\", dtype = float, delimiter=\",\", unpack=False)\n",
    "    EL = None\n",
    "    EA = None\n",
    "    \n",
    "if dataset == 'SYNTHETICnew':\n",
    "    A = np.loadtxt(path_to_data_folder + \"SYNTHETICnew/SYNTHETICnew_A.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    indicator = np.loadtxt(path_to_data_folder + \"SYNTHETICnew/SYNTHETICnew_graph_indicator.txt\", dtype = int, delimiter=\",\", unpack=False)\n",
    "    NL = None\n",
    "    NA = np.loadtxt(path_to_data_folder + \"SYNTHETICnew/SYNTHETICnew_node_attributes.txt\", dtype = float, delimiter=\",\", unpack=False)\n",
    "    EL = None\n",
    "    EA = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NL is not None:\n",
    "    if len(NL.shape) == 1:\n",
    "        NL = np.reshape(NL,(NL.shape[0],1))\n",
    "if EL is not None:\n",
    "    if len(EL.shape) == 1:\n",
    "        EL = np.reshape(EL,(EL.shape[0],1))\n",
    "if len(NA.shape) == 1:\n",
    "    NA = np.reshape(NA,(NA.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(A, indicator, NA, EA=None, NL=None, EL=None):\n",
    "    U = np.unique(indicator)\n",
    "    count=[]\n",
    "    for i in range(U.shape[0]):\n",
    "        tmp = indicator[indicator==U[i]]\n",
    "        count.append(tmp.shape[0])\n",
    "    adj=[]\n",
    "    adj_list = []\n",
    "    node_label = []\n",
    "    node_attr = []\n",
    "    edge_list = []\n",
    "    edge_label = []\n",
    "    edge_label_mat = []\n",
    "    edge_attr= []\n",
    "    penalty = 0\n",
    "    p = 0\n",
    "    for i in range(len(count)):  \n",
    "        tmp = np.zeros((count[i], count[i]))\n",
    "        tmp_E = np.zeros((count[i], count[i]))\n",
    "        flag = 1\n",
    "        breakpoint = np.sum(count[0:i+1]) \n",
    "        tmp_EA = OrderedDict()\n",
    "        while flag==1:\n",
    "            if A[p,0]<=breakpoint and A[p,1]<=breakpoint:\n",
    "                A[p,0] = A[p,0] - 1\n",
    "                A[p,1] = A[p,1] - 1\n",
    "                tmp[A[p,0]-penalty,A[p,1]-penalty] = 1\n",
    "                \n",
    "                if EL is not None: \n",
    "                    tmp_E[A[p,0]-penalty,A[p,1]-penalty] = int(EL[p])\n",
    "                else:\n",
    "                    tmp_E[A[p,0]-penalty,A[p,1]-penalty] = int(1)\n",
    "                    \n",
    "                if EA is not None:  \n",
    "                    tmp_EA[str(A[p,0]-penalty)+str(A[p,1]-penalty)] = EA[p,:]\n",
    "                p=p+1 \n",
    "                if p == A.shape[0]:\n",
    "                    adj_list_tmp = []\n",
    "                    for j in range(tmp.shape[0]):\n",
    "                        adj_list_tmp.append(np.nonzero(tmp[j,:]))\n",
    "                        \n",
    "                    indx = np.nonzero(np.triu(tmp))   \n",
    "                    tmp1 = np.array((indx[0],indx[1]))\n",
    "                    edge_list.append(tmp1.T)\n",
    "                    \n",
    "                    if NL is not None:\n",
    "                        node_label.append(NL[penalty:penalty+breakpoint,0])\n",
    "                    else:\n",
    "                        node_label.append(np.ones(([breakpoint,1])))\n",
    "                    node_attr.append(NA[penalty:penalty+breakpoint,:])\n",
    "                    adj_list.append(adj_list_tmp)\n",
    "                    adj.append(sp.sparse.csr_matrix(tmp)) \n",
    "                    edge_label.append(tmp_E[indx[0],indx[1]])\n",
    "                    edge_label_mat.append(tmp_E)\n",
    "                    if EA is not None:\n",
    "                        edge_attr.append(tmp_EA) \n",
    "                    del tmp, tmp_E\n",
    "                    flag=0\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                adj_list_tmp = []\n",
    "                for j in range(tmp.shape[0]):\n",
    "                    adj_list_tmp.append(np.nonzero(tmp[j,:]))\n",
    "                    \n",
    "                \n",
    "                indx = np.nonzero(np.triu(tmp)) \n",
    "                tmp1 = np.array((indx[0],indx[1]))\n",
    "                edge_list.append(tmp1.T)\n",
    "                \n",
    "                if NL is not None:\n",
    "                    node_label.append(NL[penalty:penalty+breakpoint,0])\n",
    "                else:\n",
    "                    \n",
    "                    node_label.append(np.ones(([breakpoint,1])))\n",
    "                node_attr.append(NA[penalty:penalty+breakpoint,:])\n",
    "                adj_list.append(adj_list_tmp)\n",
    "                adj.append(sp.sparse.csr_matrix(tmp)) \n",
    "                edge_label.append(tmp_E[indx[0],indx[1]])\n",
    "                edge_label_mat.append(tmp_E)\n",
    "                if EA is not None:\n",
    "                        edge_attr.append(tmp_EA) \n",
    "                del tmp, tmp_E\n",
    "                flag = 0 \n",
    "        flag = 1\n",
    "        penalty = breakpoint  \n",
    "    return adj, adj_list, node_label, node_attr, edge_list, edge_label,edge_label_mat, edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adj,adj_list, NL_list, NA_list, E_list, EL_list,ELM_list, EA_list = create_data(A, indicator, NA,EA, NL, EL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WL_refinement(adj_list, NL_list):\n",
    "    WL_list=[]\n",
    "    od = OrderedDict()\n",
    "    p = 0\n",
    "    for i in range(0, len(adj_list)):\n",
    "        WL_tmp=[]\n",
    "        for j in range(0,len(adj_list[i])):\n",
    "            tmp = np.sort(NL_list[i][adj_list[i][j]])\n",
    "            tmp = ''.join(str(x) for x in tmp) \n",
    "            label = str(NL_list[i][j])+tmp\n",
    "            if label in od:\n",
    "                WL_tmp.append(od[label])\n",
    "            else:\n",
    "                od[label] = p\n",
    "                WL_tmp.append(p)\n",
    "                p+=1\n",
    "        WL_list.append(np.asarray(WL_tmp))\n",
    "    return WL_list,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WL_labeling_SP(SP, ELM_list, WL_list):\n",
    "    WL_sp_add = []\n",
    "    for i in range(len(SP)):\n",
    "        ELM = ELM_list[i]\n",
    "        od = OrderedDict()\n",
    "        for j in range(len(SP[i])):\n",
    "            path = SP[i][j]  \n",
    "            pathl = WL_list[i][path] \n",
    "            pathl = pathl.tolist()\n",
    "            epath=[]\n",
    "                        \n",
    "            for n in range(len(pathl)-1):\n",
    "                epath.append(int(ELM[path[n],path[n+1]])) \n",
    "                \n",
    "            label1 = pathl + epath\n",
    "            pathl.reverse()\n",
    "            epath.reverse() \n",
    "            label2 = pathl + epath\n",
    "            \n",
    "            label1, label2 = ' '.join(str(x) for x in label1) , ' '.join(str(x) for x in label2) \n",
    "            if label1 not in od and label2 not in od:     \n",
    "                od[label1] = []    \n",
    "            if label1 in od:  \n",
    "                od[label1].append([path[0], path[-1]]) \n",
    "            if label2 in od and label2!=label1:\n",
    "                path.reverse()\n",
    "                od[label2].append([path[0], path[-1]])           \n",
    " \n",
    "        WL_sp_add.append(od)\n",
    "    return WL_sp_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_computation(common_keys,od1,od2, NA1, NA2, odEA1, odEA2, ker, beta1, beta2): \n",
    "                tmp11, tmp12, tmp21, tmp22 = np.zeros((0)), np.zeros((0)), np.zeros((0)), np.zeros((0))\n",
    "                tmpE1, tmpE2 = np.zeros((0,beta2)), np.zeros((0,beta2)) \n",
    "                for k in common_keys:\n",
    "                    lst1,lst2 = od1[k], od2[k]  \n",
    "                     \n",
    "                    if odEA1 != None:\n",
    "                        EAlst1, EAlst2= [],[] \n",
    "                        for m in range(len(lst1)):\n",
    "                            c = str(lst1[m][0])+str(lst1[m][1])\n",
    "                            EAlst1.append(odEA1[c])\n",
    "                        for m in range(len(lst2)):\n",
    "                            c = str(lst2[m][0])+str(lst2[m][1])\n",
    "                            EAlst2.append(odEA2[c])\n",
    "                        EAlst1=np.asarray(EAlst1)\n",
    "                        EAlst2=np.asarray(EAlst2) \n",
    "                        EAlst1, EAlst2 = np.repeat(EAlst1, len(EAlst2), axis=0), np.tile(EAlst2,(len(EAlst1),1))   \n",
    "                        tmpE1 = np.concatenate((tmpE1,EAlst1), axis=0)\n",
    "                        tmpE2 = np.concatenate((tmpE2,EAlst2), axis=0)\n",
    "                        \n",
    "                    lst1,lst2 = np.asarray(lst1, dtype = np.int32), np.asarray(lst2, dtype = np.int32)  \n",
    "                    lst1, lst2 = np.repeat(lst1, len(lst2), axis=0), np.tile(lst2,(len(lst1),1))   \n",
    "                    tmp11 = np.concatenate((tmp11,lst1[:,0]), axis=0)\n",
    "                    tmp12 = np.concatenate((tmp12,lst2[:,0]), axis=0)              \n",
    "                    tmp21 = np.concatenate((tmp21,lst1[:,1]), axis=0)\n",
    "                    tmp22 = np.concatenate((tmp22,lst2[:,1]), axis=0) \n",
    "                \n",
    "                if len(common_keys)!=0: \n",
    "                    N11, N12 = NA1[tmp11.astype(int)], NA2[tmp12.astype(int)]   \n",
    "                    N21, N22 = NA1[tmp21.astype(int)], NA2[tmp22.astype(int)]  \n",
    "                    if ker==0:\n",
    "                        t1 =  np.exp(-1/beta1*(np.linalg.norm((N11-N12),axis=1)**2)) \n",
    "                        t2 =  np.exp(-1/beta1*(np.linalg.norm((N21-N22),axis=1)**2))  \n",
    "                        t = np.multiply(t1, t2)   \n",
    "                        if len(EA_list)!=0:\n",
    "                            t = np.multiply(t, np.exp(-1/beta2*(np.linalg.norm((tmpE1-tmpE2),axis=1)**2)))\n",
    "                        t = np.sum(t) \n",
    "                         \n",
    "                    else:\n",
    "                        t1 =  np.multiply(N11,N12) \n",
    "                        t1 = np.sum(t1,axis = 1)\n",
    "                        t2 =  np.multiply(N21,N22) \n",
    "                        t2 = np.sum(t2,axis = 1)  \n",
    "                        t = np.multiply(t1, t2)\n",
    "                        if odEA1 != None:\n",
    "                            t1 = np.multiply(tmpE1, tmpE2)\n",
    "                            t1= np.sum(t1, axis = 1)\n",
    "                            t = np.multiply(t, t1)\n",
    "                        t = np.sum(t)  \n",
    "                else:\n",
    "                    t=0 \n",
    "                return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NP_kernel_imp2(adj, adj_list, NL_list, NA_list, E_list, EL_list,ELM_list, EA_list, h, ker): \n",
    "     \n",
    "    beta1 = NA_list[0].shape[1]\n",
    "    beta2= 1\n",
    "    if len(EA_list)!=0:\n",
    "        beta2 = EA.shape[1]\n",
    "    \n",
    "    \n",
    "#     # Finding WL edge addresses\n",
    "    WL_list, label_count = WL_refinement(adj_list, NL_list)\n",
    "\n",
    "    \n",
    "    # Finding shortest paths\n",
    "    SP = [] \n",
    "    for j in range(len(adj_list)): \n",
    "        sp = []\n",
    "        G = nx.from_scipy_sparse_matrix(adj[j]) \n",
    "        for k in range(len(adj_list[j])):\n",
    "            for m in range(len(adj_list[j])):\n",
    "                if m>k: \n",
    "                    if nx.has_path(G,k,m):\n",
    "                        path = nx.shortest_path(G,k,m) \n",
    "                        sp.append(path)\n",
    "        SP.append(sp) \n",
    "        \n",
    "        \n",
    "    # Finding out the WL labels and WL labels for shortest paths for the whole iteration and the feature dictionary\n",
    "    WL_label_list=[WL_list]\n",
    "    label_count_list = [label_count]\n",
    "    curr_list = WL_list\n",
    "    SP_WL_label_list = []\n",
    "    SP_WL_label_list.append(WL_labeling_SP(SP, ELM_list, WL_list))\n",
    "    for i in range(1,h):\n",
    "        WL_list_new, label_count_new = WL_refinement(adj_list, WL_label_list[-1])\n",
    "        WL_label_list.append(WL_list_new)\n",
    "        label_count_list.append(label_count_new)\n",
    "        SP_WL_label_list.append(WL_labeling_SP(SP, ELM_list, WL_label_list[-1]))                \n",
    "    K = np.zeros((len(adj_list), len(adj_list))) \n",
    "    odEA1 , odEA2 = None, None\n",
    "    for i in range(len(adj_list)):  \n",
    "        NA1 = NA_list[i]\n",
    "        if len(EA_list)!=0:\n",
    "                odEA1 = EA_list[i] \n",
    "        for j in range(len(adj_list)):\n",
    "            \n",
    "            if i<=j:\n",
    "                if len(EA_list)!=0:\n",
    "                    odEA2 = EA_list[j]  \n",
    "                kv1=0\n",
    "                NA2 = NA_list[j]\n",
    "                for k in range(h): \n",
    "                    kv = 0\n",
    "                    sp_od1, sp_od2 = SP_WL_label_list[k][i], SP_WL_label_list[k][j]\n",
    "                    st1, st2 = set(sp_od1), set(sp_od2) \n",
    "                    common_keys = st1.intersection(st2)\n",
    "                    if len(common_keys)!=0: \n",
    "                        kv = pairwise_computation(common_keys, sp_od1,sp_od2, NA1, NA2, odEA1, odEA2, ker, beta1, beta2)  \n",
    "                    kv1+=kv \n",
    "                K[i,j], K[j,i] = kv1,kv1   \n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "K_NPS = NP_kernel_imp2(adj, adj_list, NL_list, NA_list, E_list, EL_list,ELM_list, EA_list, iter_num, ker) \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246.3698341846466"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Time taken (in seconds): {}\".format(end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
